\documentclass[aspectratio=169, 14pt]{beamer}
\geometry{paperwidth=20cm,paperheight=11.25cm}

% Pacchetti fondamentali
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs} % Per tabelle professionali
\usepackage{multirow}
\usepackage{adjustbox}

% Tema e Colori
% Il tema "Madrid" o "Boadilla" è pulito e simile allo stile accademico standard
\usetheme{Boadilla}
\usecolortheme{beaver} % O 'dolphin'/'default' per toni blu
\graphicspath{ {./img/} }
% Personalizzazione colori per avvicinarsi al "Blu Pisa" se desiderato
\definecolor{UnipiBlue}{RGB}{0, 51, 102} 
\setbeamercolor{structure}{fg=UnipiBlue}
\setbeamercolor{title}{fg=UnipiBlue}
\setbeamercolor{frametitle}{fg=UnipiBlue}
\setbeamertemplate{caption}[numbered]
% --- INFORMAZIONI TITOLO [cite: 1, 5, 6, 7] ---
\title[ML 2025 Project]{ML 2025 Project}
\author[STFfanclub]{
	Authors: Davide Petillo, Valerio Stancanelli \\
	\medskip
	\small{STFfanclub} \\
	\medskip
	\small{Master degree (AI, Data Science)}  \\
	\small{\href{mailto:d.petillo@studenti.unipi.it}{d.petillo@studenti.unipi.it}} \\
	\small{\href{mailto:v.stancanelli1@studenti.unipi.it}{v.stancanelli1@studenti.unipi.it}}
}
\institute[]{
	\textbf{Università di Pisa} \\
	\textit{Type of project: B}
}
\date{Date 22/01/2026}

\begin{document}
	
	% --- SLIDE 1: Titolo [cite: 1] ---
	\begin{frame}[plain, noframenumbering]
		\titlepage
	\end{frame}
	
	% --- SLIDE 2: Objectives [cite: 12] ---
	\begin{frame}{Objectives}
			% Blocco Obiettivi
			\begin{block}{Main Objectives}
				\begin{itemize}
					\item \textbf{Monk Tasks:} Achieve perfect accuracy on noiseless tasks and consistency with \textbf{noise} on Monk 3.
					\item \textbf{CUP Challenge:} Exploit the specific \textbf{geometry of the data} to maximize performance.
				\end{itemize}
			\end{block}
			
			\vspace{0.5cm}
			
			% Lista Metodi
			\textbf{Explored Models \& Algorithms:}
			\begin{itemize}
				\item \textbf{Neural Networks:} Varied architectures (shallow to deep), optimizers (Adam vs SGD), and regularization schemas.
				\item \textbf{Linear Basis Expansion (LBE):} Basis chosen empirically via \textbf{Fourier Analysis}.
				\item \textbf{Other Approaches:} K-NN, Random Forest, Ensembling, Stacking, SVR.
			\end{itemize}
		\medskip
		Studying the plots and the correlations among features, lead us to make important assumptions on the geometry of the data.
	\end{frame}
	
	% --- SLIDE 3: Monk Methodology (Aggiornata) ---
	\begin{frame}{MONK: Method \& Validation}
		\textbf{Libraries used:} PyTorch, sklearn
		\vspace{0.3cm}
		
		\textbf{Model Selection Strategy:}
		\begin{itemize}
			\item \textbf{Validation Schema:} \textbf{5-Fold Cross Validation} was used for hyperparameter tuning to ensure robustness.
			\item \textbf{Metric:} Model selection based on Mean Accuracy across folds.
		\end{itemize}
		
		\vspace{0.3cm}
		\textbf{Hyperparameter Search Space (Grid Search):}
		\begin{itemize}
			\item \textbf{Architectures:} $[5], [10], [20], [10, 10], [5,5,5]$
			\item \textbf{Activations:} ReLU, Tanh
			\item \textbf{Learning Rate $\eta$:} $0.01, 0.05, 0.1, 0.2, 0.3$ \quad \textbf{Momentum $\alpha$:} $0.2 - 0.9$
			\item \textbf{Regularization $\lambda$:} $0, 8\cdot10^{-5}, 10^{-4}, 10^{-3}, 10^{-2}$
			\item \textbf{Epochs:} \(50, 100, 150, 200, 300, 500, 600, 1000\)
			\item \textbf{Initialization: } Xavier for tanh, kaimig uniform otherwise.
		\end{itemize}
		
		This grid-search took about 2 hours to run.
	\end{frame}
	
	% --- SLIDE 4: Monk Results Table (Tutti i task) ---
	\begin{frame}{MONK Results: Summary Table}
		\begin{table}[]
			\caption{CUP performance summary.}
			\centering
			\resizebox{\textwidth}{!}{
				\begin{tabular}{l|l|c|c}
					\toprule
					\textbf{Task} & \textbf{Best Conf. ($\text{Arch}, \eta, \alpha, \lambda, \text{act}, \text{\#ep}$)} & \textbf{MSE (TR / TS)} & \textbf{Acc (TR / TS) \%} \\
					\midrule
					\textbf{MONK 1} & $[20], 0.3, 0.95, 1\text{e-}4, \text{ReLU}, 300$ & $0.0013 / 0.0002$ & $\mathbf{100\%} / \mathbf{100\%}$ \\
					\midrule
					\textbf{MONK 2} & $[20], 0.2, 0.95, 8\text{e-}5, \text{ReLU}, 150$ & $0.0003 / 0.0004$ & $\mathbf{100\%} / \mathbf{100\%}$ \\
					\midrule
					\textbf{MONK 3 (Reg)} & $[10], 0.05, 0.9, 1\text{e-}4, \text{ReLU}, 600$ & $ 0.0470/ 0.0460$ & $94.3\% / \mathbf{95.8\%}$ \\
					\midrule
					MONK 3 (No-Reg) & $[5,5,5], 0.01, 0.9, 0, \text{tanh}$ & $0.0517 / 0.0501$ & $95.1\% / 95.6\%$ \\
					\bottomrule
				\end{tabular}
			}
			\caption{Table of results for MONKS}
		\end{table}
		\vspace{0.2cm}
	\end{frame}
	
	% --- SLIDE 5: Monk learning curves--
	\begin{frame}{MONK 1 \& 2 Results: Learning Curves}
		\begin{columns}
			\column{0.72\textwidth}
			\centering
			\begin{tabular}{cc}
				\includegraphics[width=0.48\textwidth, height=4.2cm, keepaspectratio]{mse_monks1.png} &
				\includegraphics[width=0.48\textwidth, height=4.2cm, keepaspectratio]{acc_monks1.png} \\
				\includegraphics[width=0.48\textwidth, height=4.2cm, keepaspectratio]{mse_monks2.png} &
				\includegraphics[width=0.48\textwidth, height=4.2cm, keepaspectratio]{acc_monks2.png}
			\end{tabular}
			
			\column{0.28\textwidth}
			\centering
			\textbf{MONK 1 \& 2 overview}\\
			\vspace{0.3cm}
			\small
			Left column shows MSE (top) and accuracy (bottom) for each dataset.
			\medskip
			
			\textbf{Top pair:} MONK 1 \\
			\textbf{Bottom pair:} MONK 2 \\
			\medskip
			
			Training and test curves overlap, indicating stable convergence and no overfitting.
		\end{columns}
	\end{frame}
	
	% --- SLIDE 7: Monk learning curves---
	\begin{frame}{MONK 3 Results: Learning Curves}
		\begin{columns}
			\column{0.5\textwidth}
			\centering
			\begin{figure} 
				\centering
			\includegraphics[width=\textwidth, height=5cm, keepaspectratio]{mse_monks3.png}
			\caption{MONK 3 (Reg) - MSE}
			\label{fig:mse-monks3}
		\end{figure}
			\column{0.5\textwidth}
			\centering
\begin{figure} 
	\centering
			\includegraphics[width=\textwidth, height=5cm, keepaspectratio]{acc_monks3.png}
			\caption{MONK 3 (Reg) - Accuracy}
			\label{fig:acc-monks3}
		\end{figure}
		\end{columns}
		\vspace{0.2cm}
		\centering
	\end{frame}
	
	% --- SLIDE 8: CUP Method - Overview ---
	\begin{frame}{CUP: From Naive to Geometric}
		\textbf{Initial Approaches (Failed):}
		\begin{itemize}
			\item Standard FFNN (Input $\to$ Output), Chaining, Error-correction NNs.
			\item Result: High error, inability to capture the underlying manifold.
		\end{itemize}
		
		\vspace{0.3cm}
		\textbf{Data Analysis Breakthrough:}
		 We found that $z = y_3 - y_4$ could determine all the other targets exactly. After some plotting, and some reconnaissance of functions, and after having fit them on the data, we ended up with the following exact equations:
		 \begin{equation*}
		 	\begin{cases}
		 		z = y_3 - y_4 \\
		 		y_1 = 0.5463 \cdot z \cdot \cos(1.1395 \cdot z) \\
		 		y_2 = 0.5463 \cdot z \cdot \sin(1.1395 \cdot z) \\
		 		y_3 + y_4 = -z \cdot \cos(2z)
		 	\end{cases}
		 \end{equation*}
			
	\end{frame}
	
	% --- SLIDE 9: CUP - The Geometry ---
	\begin{frame}{CUP: new discoveries}
		We also found a strong linear correlation between \(z\) and inputs, especially if we took the first principal component. From this point on, we focused on predicting \(z\).
		\begin{itemize}
			\item Linear Regression on $z$ using inputs failed (MAE $\approx 1$).
			\item using NN,  Random Forest, SVR failed
			\item k-NN did not fail (MAE \(\approx 0.8\), still not enough but better)
		\end{itemize}
		
		So we tried k-NN in a lot of variants, using PCA, using products of the inputs, products of the PC, but nothing improved. 
		
		\vspace{0.5cm}
		
		But after some time the pseudo-success of k-NN gave us an idea: "Maybe the manifold has some local structure which is predictable, but that can not  be seen if looking at the whole picture".
	\end{frame}
	
	% --- SLIDE 10: CUP - Intermediate Models ---
	\begin{frame}{CUP: Other geometric discoveries}
	That idea lead us to zoom on the plot of z against other variables, and what we found was very good.
		\begin{figure}
		\centering
		\includegraphics[width=\textwidth, height=5cm, keepaspectratio]{z-pc1.png}
		\includegraphics[width=\textwidth, height=5cm, keepaspectratio]{z-pc2.png}
		\caption{Plot of \(z\) against PC1 (left) and PC2 (right)}
		\label{fig:z-PC1-PC2}
		\end{figure}
		
		There were oscillations! And they seemed to be periodic.
		
	\end{frame}
	
	% --- SLIDE 11: CUP - Spectral Analysis ---
	\begin{frame}{CUP: The inverse idea}
		Noticing that there seemed to be a relationship of the kind pc2\(=f(z)\), made us think about reversing the process of prediction.
		We train a model to predict a certain representation of the input and then we try to invert the process to get the correct \(z\).
		\vspace{0.5cm}
		NN and other common models didnt work, but one did.
		\textbf{Random Forest (Inverse Model):}
		\begin{itemize}
			\item Idea: Train $RF(z) \to \text{First K Principal Components of } x$.
			\item Result: MEE dropped to $\approx 12$ with \(K=6\)  and 1000 estimators.
			\item \textit{Limitation:} Worked well only in dense data regions. Needed a continuous function representation.
		\end{itemize}
		
		
	\end{frame}
	
	% --- SLIDE 12: CUP - Spectral Analysis --
	\begin{frame}{CUP: The residual analysis}
		After some time spent trying to improve the models above, we noticed that other principal components seemed to be oscillating, so our intuition was: "Maybe every input has an oscillating component apart from the linear one...".
		
		\vspace{0.1cm}
		
	\textbf{Residual Analysis:}
	We analyzed the residuals of the inputs $x_i$ when approximated linearly with respect to $z$.
	
	\begin{columns}
		\column{0.6\textwidth}
		\begin{figure}
			\centering
			\includegraphics[width=\textwidth, height=4cm, keepaspectratio]{residuals_fourier_analysis.jpg}
			\caption{Fourier Analysis of the residuals of the input \(x_i\) approximated linearly with \(z\).}
			\label{fig:res-fourier}
		\end{figure}
		\column{0.4\textwidth}
		\centering
		\begin{itemize}
			\item Fourier analysis revealed a clear \textbf{Fundamental Frequency} $\omega = 0.57$.
			\item Inputs are not just noisy; they contain systematic harmonic components dependent on $z$.
			\item This justified the use of a basis expansion model.
		\end{itemize}
		
	\end{columns}
	\end{frame}
	
	% --- SLIDE 13: CUP - Final Model Architecture ---
	\begin{frame}{CUP: LBE Inverse Solver}
		We model each input $x_i$ as a function of $z$ using \textbf{Linear Basis Expansion (LBE)}:
		$$ \hat{x}_i(z) = c_{i,0} z + \sum_{k=1}^{K} \left( a_{i,k} \sin(k \omega z) + b_{i,k} \cos(k \omega z) \right) $$
		
		\textbf{Components:}
		\begin{itemize}
			\item Linear trend ($z$).
			\item K Harmonics of the fundamental frequency $\omega = 0.57$.
			\item Weights $w_i = 1/MSE_i$ calculated during training to penalize noisy inputs.
		\end{itemize}
	\end{frame}
	
	% --- SLIDE 14: CUP - Inference + Refinement ---
	\begin{frame}{CUP: Inference and Refinement}
		To predict $z_{pred}$ given a new input vector $\mathbf{x}_{new}$:
		
		\begin{enumerate}
			\item \textbf{Coarse Grid Search:}
			Evaluate the weighted error function $E(z)$ over $z \in [-70, 50]$ (discretized):
			$$ z_{init} = \arg\min_z \sum_{i=1}^{12} w_i (\hat{x}_i(z) - x_{new,i})^2 $$
			
			\item \textbf{Fine Refinement:}
			Apply a Newton-based optimization starting from $z_{init}$ to find the local minimum.
			
			\item \textbf{Target Reconstruction:}
			Compute $y_1, y_2, y_3, y_4$ using the geometric equations (Slide 8) with the refined $z$.
		\end{enumerate}

		\textbf{pc2 refinement:} estimate $pc2$ via PCA and minimize $(f(z)-pc2)^2 + \lambda (z-z_{init})^2$ to obtain $z_{refined}$.
	\end{frame}

	% --- SLIDE 16: CUP - Ensemble z_ens ---
	\begin{frame}{CUP: Ensemble $z_{ens}$}
		We combine two estimates when they agree:
		\begin{itemize}
			\item if $|z_{refined} - z_{forest\_refined}| < \alpha$, use their weighted average
			\item otherwise keep $z_{refined}$
		\end{itemize}
		\begin{columns}
			\column{0.5\textwidth}
			\begin{figure}
				\centering
				\includegraphics[width=\textwidth, height=4cm, keepaspectratio]{abs_z_error_z_refined_vs_z_forest_refined.png}
				\caption{Error comparison: $z_{refined}$ vs $z_{forest\_refined}$.}
			\end{figure}
			\column{0.5\textwidth}
			$z_{refined}$ is the value given by the LBE inverse solver, and $z_{forest\_refined}$ is the value given by a random forest model. This last model has higher MEE, so we give it low weight, and we use it only when we estimate it's accurate enough (i.e., close to $z_{refined}$).
		\end{columns}
	\end{frame}
	
	% --- SLIDE 17: CUP - Validation Schema ---
	\begin{frame}{CUP: Validation Schema}
		\textbf{Data Splitting:}
		\begin{itemize}
			\item \textbf{Internal Test Set:} 10\% of the full training set is held out.
			\item \textbf{Cross Validation:} 5-fold CV is run on the remaining 90\% for model selection.
			\item \textbf{Final Evaluation:} selected hyperparameters are evaluated once on the internal test set.
		\end{itemize}
		
		\begin{center}
			\textit{(Graphic representation of 5-Fold CV on 90\%)} \\
			\fbox{Fold 1 (VL)} \fbox{Fold 2 (TR)} \fbox{Fold 3 (TR)} \fbox{Fold 4 (TR)} \fbox{Fold 5 (TR)} ...
		\end{center}
	\end{frame}
	
	% --- SLIDE 18: CUP - Model Selection ---
	\begin{frame}{CUP: Model Selection}
		\textbf{Hyperparameters Tuned (Grid Search):}
		\begin{itemize}
			\item \textbf{Refinement $\lambda$:} regularization for $z$ refinement via $pc2$.
			\item \textbf{Ensemble weight $\gamma$:} mixing coefficient between $z_{refined}$ and $z_{forest\_refined}$.
			\item \textbf{Agreement threshold $\alpha$:} decides when the ensemble average is used.
		\end{itemize}
		
		\textbf{Selection Criterion:}
		\begin{itemize}
			\item pick the combination with the lowest average MEE across CV folds.
		\end{itemize}

		\textbf{Grid Search:}
		\begin{itemize}
			\item $\lambda$ $\in$ \{0.01, 0.02, 0.05, 0.1, 0.2, 0.3, 0.5, 1.0, 2.0, 5.0\}
			\item $\gamma$ $\in$ \{0.05, 0.1, 0.15, 0.2, 0.3\}
			\item $\alpha$ $\in$ \{0.3, 0.6, 1.0, 1.5, 2.0, 3.0\}
		\end{itemize}

		The best hyperparameters found (with lowest MEE on the validation set) are the following: $\lambda = 1.0$, $\gamma = 0.1$, $\alpha = 1.5$. This model clearly outperforms the other models we have tried.
	\end{frame}
	
	% --- SLIDE 19: CUP - Grid Search Summary ---
	\begin{frame}{CUP: Grid Search Summary}
		\textbf{Total combinations:} 300
		\begin{table}[]
			\centering
			\begin{tabular}{c|c|c|c|c|c|c}
				\toprule
				\textbf{Rank} & $\lambda$ & $\gamma$ & $\alpha$ & \textbf{Train (avg $\pm$ std)} & \textbf{Val (avg $\pm$ std)} & \textbf{Test (avg)} \\
				\midrule
				1 & 1.00 & 0.10 & 1.50 & 6.62 $\pm$ 0.33 & 8.48 $\pm$ 0.97 & 7.84 \\
				2 & 1.00 & 0.10 & 3.00 & 6.63 $\pm$ 0.33 & 8.49 $\pm$ 0.99 & 7.87 \\
				5 & 1.00 & 0.15 & 1.00 & 6.55 $\pm$ 0.32 & 8.53 $\pm$ 1.06 & 7.82 \\
				20 & 2.00 & 0.10 & 1.00 & 6.40 $\pm$ 0.29 & 8.56 $\pm$ 0.96 & 7.73 \\
				60 & 5.00 & 0.05 & 1.50 & 6.43 $\pm$ 0.31 & 8.65 $\pm$ 0.91 & 7.72 \\
				300 & 0.02 & 0.30 & 3.00 & 7.79 $\pm$ 0.40 & 9.36 $\pm$ 1.21 & 8.39 \\
				\bottomrule
			\end{tabular}
		\end{table}
	\end{frame}
	
	% --- SLIDE 19: CUP - Final Results Table ---
	\begin{frame}{CUP: Final Results}
		\textbf{Model:} Geometric Inverse Solver (LBE + Newton Refinement)
		
		\begin{table}[]
			\centering
			\large
			\begin{tabular}{l|c|c}
				\toprule
				\textbf{Dataset Partition} & \textbf{MEE (Mean)} & \textbf{Std. Dev} \\
				\midrule
				Training & \textbf{6.620} & $\pm \mathbf{0.328}$ \\
				Validation & \textbf{8.482} & $\pm \mathbf{0.965}$ \\
				Internal Test & \textbf{7.843} & -- \\
				\bottomrule
			\end{tabular}
			\caption{CUP performance summary.}
		\end{table}
		
		\vspace{0.5cm}
		\textbf{Computing Time:} approximately one minute in total.
		\textbf{Hardware:} Standard CPU (No GPU required).
	\end{frame}
	
	% --- SLIDE 18: Conclusions ---
	\begin{frame}{Conclusions}
		\textbf{Summary:}
		\begin{itemize}
			\item \textbf{MONK:} Confirmed that procedural complexity handling implies success on simpler tasks.
			\item \textbf{CUP:} Demonstrated that domain analysis (geometry) can vastly outperform black-box optimization. 
			\item Achieved a reasonable performance with MEE $\approx 8.5$.
		\end{itemize}
		
		\vspace{1cm}
		\textbf{Blind Test Results:}
		\begin{itemize}
			\item Filename: \texttt{STFfanclub\_ML-CUP25-TS.csv}
			\item Nickname: \textbf{STFfanclub}
		\end{itemize}
	\end{frame}
	
	% --- APPENDIX ---
	\begin{frame}{Appendix A: Additional Plots}
		\begin{figure}
			\centering
			\includegraphics[width=\textwidth, height=4cm, keepaspectratio]{mee_alpha_gamma.png}
			\caption{MEE vs $\alpha$ for a fixed $\gamma$, and MEE vs $\gamma$ for a fixed $\alpha$.}
		\end{figure}

		$z_{forest\_refined}$ has a relatively high MEE, and when it is far from $z_{refined}$ it is most likely wrong. However, we can still improve the final MEE by giving it a low weight and using it only when the two values of $z$ are close to each other.
	\end{frame}
	
	% --- INIZIO FRAME APPENDICE ---
	
% --- INIZIO FRAME APPENDIX A (NN BRUTE FORCE) ---

\begin{frame}{Appendix B: Neural Network Limits}
	\framesubtitle{Exhaustive Grid Search confirms MEE plateau at $\approx$ 21.4}
	
	\begin{columns}[T]
		
		% Colonna 1: Heatmap (La prova dell'esaustività)
		\begin{column}{0.32\textwidth}
			\centering
			% Assicurati di avere nn_heatmap_exhaustive.png dallo script di grid search
			% Se non ce l'hai, usa nn_best_learning_curve.png
			\includegraphics[width=0.9\linewidth]{nn_heatmap_exhaustive.png}
			\footnotesize{\textbf{Architecture Search}\\
				Tested 60+ configurations (Depth, Width, Act., LR, momentum, dropout). No architecture breaks the MEE 20 barrier.}
		\end{column}
		
		% Colonna 2: True vs Pred
		\begin{column}{0.32\textwidth}
			\centering
			\includegraphics[width=0.9\linewidth]{nn_best_pred_vs_true.png}
			\footnotesize{\textbf{Linear Bias}\\
				Best Model ([100,100,100], ReLU, lr=1e-4, \(\alpha\)=0.1) still fails to track the high-frequency geometric manifold.}
		\end{column}
		
		% Colonna 3: Residui
		\begin{column}{0.32\textwidth}
			\centering
			\includegraphics[width=0.9\linewidth]{nn_best_learning_curve.png}
			\footnotesize{\textbf{Learning Curve}}
		\end{column}
		
	\end{columns}
	
	\vspace{0.5cm}
	
	\begin{alertblock}{Deep Learning Conclusion}
		Even with deep architectures and aggressive tuning, MLPs suffer from \textbf{Spectral Bias} on this dataset. They learn the trend but cannot reconstruct the specific geometric curve from only 500 samples.
	\end{alertblock}
	
\end{frame}

% --- INIZIO FRAME APPENDIX B (Z VARIABLE UPDATED) ---

\begin{frame}{Appendix C: The "Z-Variable" Bottleneck}	
	\begin{columns}[T]
		
		% Colonna 1: Learning Curve
		\begin{column}{0.32\textwidth}
			\centering
			\includegraphics[width=\linewidth]{nn_z_learning_curve.png}
			\footnotesize{\textbf{A. Optimization Limit}\\
				Grid Search (60 configs) confirms that NNs cannot reduce MAE below 1 on the latent variable $z$.} Best: [200,100], ReLU, \(\alpha=0.01\), lr=1e-5
		\end{column}
		
		% Colonna 2: True vs Pred
		\begin{column}{0.32\textwidth}
			\centering
			\includegraphics[width=\linewidth]{nn_z_pred_vs_true.png}
			\footnotesize{\textbf{B. Approximation Error}\\
				The model approximates the $z$-manifold roughly but fails to capture the exact curvature.}
		\end{column}
		
		% Colonna 3: Residuals
		\begin{column}{0.32\textwidth}
			\centering
			\includegraphics[width=\linewidth]{nn_z_residuals.png}
			\footnotesize{\textbf{C. Structured Residuals}\\
				Deterministic patterns in residuals prove that geometric information is missing from the model.}
		\end{column}
		
	\end{columns}
	
	\vspace{0.5cm}
	
	\begin{alertblock}{Geometric Insight}
		The variable $z$ encodes high-frequency variations. Standard MLPs act as \textbf{Low-Pass Filters}, smoothing out exactly the information needed to reconstruction the full target vector $y$.
	\end{alertblock}
	
\end{frame}

% --- FINE FRAME APPENDIX B ---

\begin{frame}{Appendix D: Exhaustive SVR Analysis}
	\framesubtitle{Grid search confirms structural limits (Best MEE $\approx$ 17.65)}
	
	\begin{columns}[T]
		
		% Colonna 1: Heatmap (La prova che avete cercato ovunque)
		\begin{column}{0.32\textwidth}
			\centering
			\includegraphics[width=\linewidth]{svr_heatmap_exhaustive.jpg}
			\footnotesize{\textbf{A. The "Error Wall"}\\
				Grid search across 6 orders of magnitude ($C \in [1, 10^5]$) finds no solution under MEE 17.}
		\end{column}
		
		
		\begin{column}{0.32\textwidth}
			\centering
			\includegraphics[width=\linewidth]{svr_best_pred_vs_true.png}
			\footnotesize{\textbf{B. Kernel Smoothing}\\
				Even with optimal parameters (\(C=10, \gamma=1, \varepsilon=0.1\)), the RBF kernel smooths out high-frequency geometric details.}
		\end{column}
		
		% Colonna 3: Residuals (La prova dell'errore non casuale)
		\begin{column}{0.32\textwidth}
			\centering
			\includegraphics[width=\linewidth]{svr_best_residuals.png}
			\footnotesize{\textbf{C. Residuals}}
		\end{column}
		
	\end{columns}
	
	\vspace{0.5cm}
	

	
\end{frame}

\begin{frame}{Appendix E: Conclusion on "standard" approaches}
		\begin{alertblock}{Final Verdict on Standard Methods}
		Neither Deep Learning (MEE $\approx$ 21) nor Kernel Machines (MEE $\approx$ 17.6) can solve the CUP. The problem requires explicit \textbf{Geometric Feature Engineering} (LBE), which achieves MEE $\mathbf{7.8}$.
	\end{alertblock}
\end{frame}


	
	\end{document}
