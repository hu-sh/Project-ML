\documentclass[aspectratio=169, 10pt]{beamer}

% Pacchetti fondamentali
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs} % Per tabelle professionali
\usepackage{multirow}
\usepackage{adjustbox}

% Tema e Colori
% Il tema "Madrid" o "Boadilla" è pulito e simile allo stile accademico standard
\usetheme{Boadilla}
\usecolortheme{beaver} % O 'dolphin'/'default' per toni blu
\graphicspath{ {./img/} }
% Personalizzazione colori per avvicinarsi al "Blu Pisa" se desiderato
\definecolor{UnipiBlue}{RGB}{0, 51, 102} 
\setbeamercolor{structure}{fg=UnipiBlue}
\setbeamercolor{title}{fg=UnipiBlue}
\setbeamercolor{frametitle}{fg=UnipiBlue}
\setbeamertemplate{caption}[numbered]
% --- INFORMAZIONI TITOLO [cite: 1, 5, 6, 7] ---
\title[ML 2025 Project]{ML 2025 Project}
\author[TODO]{
	Authors: Davide Petillo, Valerio Stancanelli \\
	\medskip
	\small{TODO} \\
	\medskip
	\small{Master degree (AI, Data Science)}  \\
	\small{\href{mailto:d.petillo@studenti.unipi.it}{d.petillo@studenti.unipi.it}} \\
	\small{\href{mailto:v.stancanelli1@studenti.unipi.it}{v.stancanelli1@studenti.unipi.it}}
}
\institute[]{
	\textbf{Università di Pisa} \\
	\textit{Type of project: B}
}
\date{Date 23/01/2026}

\begin{document}
	
	% --- SLIDE 1: Titolo [cite: 1] ---
	\begin{frame}
		\titlepage
	\end{frame}
	
	% --- SLIDE 2: Objectives [cite: 12] ---
	\begin{frame}{Objectives}
			% Blocco Obiettivi
			\begin{block}{Main Objectives}
				\begin{itemize}
					\item \textbf{Monk Tasks:} Achieve perfect accuracy on noiseless tasks and consistency with \textbf{noise} on Monk 3.
					\item \textbf{CUP Challenge:} Exploit the specific \textbf{geometry of the data} to maximize performance.
				\end{itemize}
			\end{block}
			
			\vspace{0.5cm}
			
			% Lista Metodi
			\textbf{Explored Models \& Algorithms:}
			\begin{itemize}
				\item \textbf{Neural Networks:} Varied architectures (shallow to deep), optimizers (Adam vs SGD), and regularization schemas.
				\item \textbf{Linear Basis Expansion (LBE):} Basis chosen empirically via \textbf{Fourier Analysis}.
				\item \textbf{Other Approaches:} K-NN, Random Forest, Ensembling, Stacking, SVR.
			\end{itemize}
		\medskip
		Studying the plots and the correlations among features, lead us to make important assumptions on the geometry of the data.
	\end{frame}
	
	% --- SLIDE: Monk Methodology (Aggiornata) ---
	\begin{frame}{2. MONK: Method \& Validation}
		\textbf{Model Selection Strategy:}
		\begin{itemize}
			\item \textbf{Validation Schema:} \textbf{5-Fold Cross Validation} was used for hyperparameter tuning to ensure robustness.
			\item \textbf{Metric:} Model selection based on Mean Accuracy across folds.
		\end{itemize}
		
		\vspace{0.3cm}
		\textbf{Hyperparameter Search Space (Grid Search):}
		\begin{itemize}
			\item \textbf{Architectures:} $[5], [10], [20], [10, 10], [5,5,5]$
			\item \textbf{Activations:} ReLU, Tanh
			\item \textbf{Learning Rate $\eta$:} $0.01, 0.05, 0.1, 0.2, 0.3$ \quad \textbf{Momentum $\alpha$:} $0.2 - 0.9$
			\item \textbf{Regularization $\lambda$:} $0, 8\cdot10^{-5}, 10^{-4}, 10^{-3}, 10^{-2}$
			\item \textbf{Epochs:} \(50, 100, 150, 200, 300, 500, 600, 1000\)
		\end{itemize}
		
		This grid-search took about 2 hours to run.
	\end{frame}
	
	% --- SLIDE: Monk Results Table (Tutti i task) ---
	\begin{frame}{3.1 MONK Results: Summary Table}
		\begin{table}[]
			\centering
			\resizebox{\textwidth}{!}{
				\begin{tabular}{l|l|c|c}
					\toprule
					\textbf{Task} & \textbf{Best Conf. ($\text{Arch}, \eta, \alpha, \lambda, \text{act}, \text{\#ep}$)} & \textbf{MSE (TR / TS)} & \textbf{Acc (TR / TS) \%} \\
					\midrule
					\textbf{MONK 1} & $[20], 0.3, 0.95, 1\text{e-}4, \text{ReLU}, 300$ & $0.0013 / 0.0002$ & $\mathbf{100\%} / \mathbf{100\%}$ \\
					\midrule
					\textbf{MONK 2} & $[20], 0.2, 0.95, 8\text{e-}5, \text{ReLU}, 150$ & $0.0003 / 0.0004$ & $\mathbf{100\%} / \mathbf{100\%}$ \\
					\midrule
					\textbf{MONK 3 (Reg)} & $[10], 0.05, 0.9, 1\text{e-}4, \text{ReLU}, 600$ & $ 0.0470/ 0.0460$ & $94.3\% / \mathbf{95.8\%}$ \\
					\midrule
					MONK 3 (No-Reg) & $[5,5,5], 0.01, 0.9, 0, \text{tanh}$ & $0.0517 / 0.0501$ & $95.1\% / 95.6\%$ \\
					\bottomrule
				\end{tabular}
			}
		\end{table}
		\vspace{0.2cm}
	\end{frame}
	
	\begin{frame}{3.2 MONK 1 Results: Learning Curves}
		\begin{columns}
			\column{0.5\textwidth}
			\centering
			% Inserisci qui il tuo file mse_monks2.png
			\begin{figure}
				\centering
				\includegraphics[width=\textwidth, height=5cm, keepaspectratio]{mse_monks1.png}
				\caption{MONK 1 - MSE (TR vs TS)}
				\label{fig:mse-monks1}
			\end{figure}
			
			\column{0.5\textwidth}
			\centering
			\begin{figure}
				\centering
				\includegraphics[width=\textwidth, height=5cm, keepaspectratio]{acc_monks1.png}
				\caption{MONK 1 - Accuracy (TR vs TS)}
				\label{fig:acc-monks1}
			\end{figure}
			
		\end{columns}
		\vspace{0.2cm}
		\centering
		\small \textit{Perfect convergence achieved with Tanh activation and $[10]$ hidden units.}
	\end{frame}
	% --- SLIDE: Monk 2 Plots (L'esempio perfetto) ---
	\begin{frame}{3.3 MONK 2 Results: Learning Curves}
		\begin{columns}
			\column{0.5\textwidth}
			\centering
			\begin{figure}
				\centering
			\includegraphics[width=\textwidth, height=5cm, keepaspectratio]{mse_monks2.png}
			\caption{MONK 2 - MSE (TR vs TS)}
			\label{fig:mse-monks2}
			\end{figure}
			\column{0.5\textwidth}
			\centering
			\begin{figure} 
				\centering
			\includegraphics[width=\textwidth, height=5cm, keepaspectratio]{acc_monks2.png}
			\caption{MONK 2 - Accuracy (TR vs TS)}
			\label{fig:acc-monks2}
		\end{figure}
		\end{columns}
		\vspace{0.2cm}
		\centering
		\small \textit{Perfect convergence achieved with Tanh activation and $[10]$ hidden units.}
	\end{frame}
	
	% --- SLIDE: Monk 3 Plots (L'effetto della regolarizzazione) ---
	\begin{frame}{3.4 MONK 3 Results}
		\begin{columns}
			\column{0.5\textwidth}
			\centering
			\begin{figure} 
				\centering
			\includegraphics[width=\textwidth, height=5cm, keepaspectratio]{mse_monks3.png}
			\caption{MONK 3 (Reg) - MSE}
			\label{fig:mse-monks3}
		\end{figure}
			\column{0.5\textwidth}
			\centering
\begin{figure} 
	\centering
			\includegraphics[width=\textwidth, height=5cm, keepaspectratio]{acc_monks3.png}
			\caption{MONK 3 (Reg) - Accuracy}
			\label{fig:acc-monks3}
		\end{figure}
		\end{columns}
		\vspace{0.2cm}
		\centering
		\small \textit{Note: TS Accuracy (Blue) surpasses TR Accuracy (Red), indicating successful noise handling.}
	\end{frame}
	% --- SLIDE 6: CUP Method - Overview ---
	\begin{frame}{4. CUP: From Naive to Geometric}
		\textbf{Initial Approaches (Failed):}
		\begin{itemize}
			\item Standard FFNN (Input $\to$ Output), Chaining, Error-correction NNs.
			\item Result: High error, inability to capture the underlying manifold.
		\end{itemize}
		
		\vspace{0.3cm}
		\textbf{Data Analysis Breakthrough:}
		 We found that $z = y_3 - y_4$ could determine all the other targets exactly. After some plotting, and some reconnaissance of functions, and after having fit them on the data, we ended up with the following exact equations:
		 \begin{equation*}
		 	\begin{cases}
		 		z = y_3 - y_4 \\
		 		y_1 = 0.5463 \cdot z \cdot \cos(1.1395 \cdot z) \\
		 		y_2 = 0.5463 \cdot z \cdot \sin(1.1395 \cdot z) \\
		 		y_3 + y_4 = -z \cdot \cos(2z)
		 	\end{cases}
		 \end{equation*}
			
	\end{frame}
	
	% --- SLIDE 7: CUP - The Geometry ---
	\begin{frame}{4.1 CUP: new discoveries}
		We also found a strong linear correlation between \(z\) and inputs, especially if we took the first principal component. From this point on, we focused on predicting \(z\).
		\begin{itemize}
			\item Linear Regression on $z$ using inputs failed (MAE $\approx 1$).
			\item using NN, Random Forest, SVR failed
			\item k-NN did not fail (MAE \(\approx 0.8\), still not enough but better)
		\end{itemize}
		
		So we tried k-NN in a lot of variants, using PCA, using products of the inputs, products of the PC, but nothing improved. 
		
		\vspace{0.5cm}
		
		But after some time the pseudo-success of k-NN gave us an idea: "Maybe the manifold has some local structure which is predictable, but that can not  be seen if looking at the whole picture".
	\end{frame}
	
	% --- SLIDE 8: CUP - Intermediate Models ---
	\begin{frame}{4.2 CUP: Other geometric discoveries}
	That idea lead us to zoom on the plot of z against other variables, and what we found was very good.
		\begin{figure}
		\centering
		\includegraphics[width=\textwidth, height=5cm, keepaspectratio]{z-pc2.png}
		\includegraphics[width=\textwidth, height=5cm, keepaspectratio]{z-pc1.png}
		\caption{Plot of \(z\) against PC1 and PC2}
		\label{fig:z-PC1-PC2}
		\end{figure}
		
		There were oscillations! And they seemed to be periodic.
		
	\end{frame}
	
	% --- SLIDE 9: CUP - Spectral Analysis ---
	\begin{frame}{4.3 CUP: The inverse idea}
		Noticing that there seemed to be a relationship of the kind pc2\(=f(z)\), made us think about reversing the process of prediction.
		We train a model to predict a certain representation of the input and then we try to invert the process to get the correct \(z\).
		\vspace{0.5cm}
		NN and other common models didnt work, but one did.
		\textbf{Random Forest (Inverse Model):}
		\begin{itemize}
			\item Idea: Train $RF(z) \to \text{First K Principal Components of } x$.
			\item Result: MEE dropped to $\approx 12$ with \(K=6\).
			\item \textit{Limitation:} Worked well only in dense data regions. Needed a continuous function representation.
		\end{itemize}
		
		
	\end{frame}
	
	\begin{frame}{4.4 CUP: The residual analysis}
		After some time spent trying to improve the models above, we noticed that other principal components seemed to be oscillating, so our intuition was: "Maybe every input has an oscillating component apart from the linear one...".
		
		\vspace{0.1cm}
		
	\textbf{Residual Analysis:}
	We analyzed the residuals of the inputs $x_i$ when approximated linearly with respect to $z$.
	
	\begin{columns}
		\column{0.6\textwidth}
		\begin{figure}
			\centering
			\includegraphics[width=\textwidth, height=4cm, keepaspectratio]{residuals_fourier_analysis.jpg}
			\caption{Fourier Analysis of the residuals of the input \(x_i\) approximated linearly with \(z\).}
			\label{fig:res-fourier}
		\end{figure}
		\column{0.4\textwidth}
		\centering
		\begin{itemize}
			\item Fourier analysis revealed a clear \textbf{Fundamental Frequency} $\omega = 0.57$.
			\item Inputs are not just noisy; they contain systematic harmonic components dependent on $z$.
			\item This justified the use of a basis expansion model.
		\end{itemize}
		
	\end{columns}
	\end{frame}
	% --- SLIDE 10: CUP - Final Model Architecture ---
	\begin{frame}{4.4 CUP: LBE Inverse Solver}
		We model each input $x_i$ as a function of $z$ using \textbf{Linear Basis Expansion (LBE)}:
		$$ \hat{x}_i(z) = c_{i,0} z + \sum_{k=1}^{K} \left( a_{i,k} \sin(k \omega z) + b_{i,k} \cos(k \omega z) \right) $$
		
		\textbf{Components:}
		\begin{itemize}
			\item Linear trend ($z$).
			\item K Harmonics of the fundamental frequency $\omega = 0.57$.
			\item Weights $w_i = 1/MSE_i$ calculated during training to penalize noisy inputs.
		\end{itemize}
	\end{frame}
	
	% --- SLIDE 11: CUP - Inference Algorithm ---
	\begin{frame}{4.5 CUP: Inference (Finding $z$)}
		To predict $z_{pred}$ given a new input vector $\mathbf{x}_{new}$:
		
		\begin{enumerate}
			\item \textbf{Coarse Grid Search:}
			Evaluate the weighted error function $E(z)$ over $z \in [-70, 50]$ (discretized):
			$$ z_{init} = \arg\min_z \sum_{i=1}^{12} w_i (\hat{x}_i(z) - x_{new,i})^2 $$
			
			\item \textbf{Fine Refinement:}
			Apply a Newton-based optimization starting from $z_{init}$ to find the local minimum.
			
			\item \textbf{Target Reconstruction:}
			Compute $y_1, y_2, y_3, y_4$ using the geometric equations (Slide 8) with the refined $z$.
		\end{enumerate}
	\end{frame}

		% --- SLIDE 12: CUP - Refinement via pc2 ---
	\begin{frame}{4.6 CUP: Refining $z$ with pc2}
		We refine the coarse estimate $z_{init}$ by exploiting the empirical relation $pc2 \approx f(z)$:
		\begin{itemize}
			\item compute $pc2$ from the input via PCA
			\item refine $z$ by minimizing $(f(z)-pc2)^2 + \lambda (z-z_{init})^2$
			\item obtain $z_{refined}$ used for final target reconstruction
		\end{itemize}
		\begin{figure}
			\centering
			\includegraphics[width=\textwidth, height=4cm, keepaspectratio]{z-pc2.png}
			\caption{Relationship between $z$ and $pc2$ used for refinement.}
		\end{figure}
	\end{frame}

	% --- SLIDE 13: CUP - Ensemble z_ens ---
	\begin{frame}{4.7 CUP: Ensemble $z_{ens}$}
		We combine two estimates when they agree:
		\begin{itemize}
			\item if $|z_{refined} - z_{forest\_refined}| < \alpha$, use their weighted average
			\item otherwise keep $z_{refined}$
		\end{itemize}
		\begin{columns}
			\column{0.5\textwidth}
			\begin{figure}
				\centering
				\includegraphics[width=\textwidth, height=3cm, keepaspectratio]{alpha_vs_mee_gamma_0.1.png}
				\caption{MEE vs $\alpha$ for averaging.}
			\end{figure}
			\column{0.5\textwidth}
			\begin{figure}
				\centering
				\includegraphics[width=\textwidth, height=3cm, keepaspectratio]{abs_z_error_z_refined_vs_z_forest_refined.png}
				\caption{Error comparison: $z_{refined}$ vs $z_{forest\_refined}$.}
			\end{figure}
		\end{columns}

		$z_{refined}$ is the value given by the LBE inverse solver, and $z_{forest\_refined}$ is the value given by a random forest model. This last model has higher MEE, so we give it low weight, and we use it only when we estimate it's accurate enough (i.e., close to $z_{refined}$).
	\end{frame}
	
	% --- SLIDE 12: CUP - Validation Schema ---
	\begin{frame}{5.1 CUP: Validation Schema [*]}
		\textbf{Data Splitting:}
		\begin{itemize}
			\item \textbf{5-Fold Cross Validation} used for Model Assessment and Hyperparameter tuning.
			\item Rationale: Maximized the use of data to capture the geometric manifold across the entire domain.
			\item \textbf{Internal Test Set:} Not used (All provided data used for CV to ensure manifold coverage). Blind test performance is the ultimate validation.
		\end{itemize}
		
		\begin{center}
			\textit{(Graphic representation of 5-Fold CV)} \\
			\fbox{Fold 1 (VL)} \fbox{Fold 2 (TR)} \fbox{Fold 3 (TR)} \fbox{Fold 4 (TR)} \fbox{Fold 5 (TR)} ...
		\end{center}
	\end{frame}
	
	% --- SLIDE 13: CUP - Model Selection ---
	\begin{frame}{5.2 CUP: Model Selection [*]}
		\textbf{Hyperparameters Explored:}
		\begin{itemize}
			\item \textbf{Grid Search Resolution:} Step sizes for $z$ scan (0.1, 0.01, 0.001).
			\item \textbf{Number of Harmonics:} Tried 4, 6, 8, 10. Selected \textbf{6} (Minimizes VL MEE).
			\item \textbf{Fundamental Frequency $\omega$:} Optimized via gradient descent on residuals, fixed at \textbf{0.57}.
		\end{itemize}
		
		\textbf{Final Selection Criteria:}
		\begin{itemize}
			\item Model selected based on stability of the inverse mapping (low variance in CV folds).
			\item Robustness check: Verified on data subsets to ensure no leakage.
		\end{itemize}
	\end{frame}
	
	% --- SLIDE 14: CUP - Final Results Table ---
	\begin{frame}{5.3 CUP: Final Results [*]}
		\textbf{Model:} Geometric Inverse Solver (LBE + Newton Refinement)
		
		\begin{table}[]
			\centering
			\large
			\begin{tabular}{l|c|c}
				\toprule
				\textbf{Dataset Partition} & \textbf{MEE (Mean)} & \textbf{Std. Dev} \\
				\midrule
				Training (TR) & 8.2 & $\pm 0.8$ \\
				Validation (VL - 5 Fold CV) & \textbf{8.5} & $\pm \mathbf{1.0}$ \\
				\bottomrule
			\end{tabular}
		\end{table}
		
		\vspace{0.5cm}
		\textbf{Computing Time:} $< 1$ second per sample for inference (highly efficient).
		\textbf{Hardware:} Standard CPU (No GPU required).
	\end{frame}
	
	% --- SLIDE 15: CUP - Error Plots ---
	\begin{frame}{5.4 CUP: Error Analysis [*]}
		\begin{columns}
			\column{0.5\textwidth}
			\centering
			% [PLACEHOLDER]
			\fbox{\begin{minipage}[c][4cm]{5cm} \centering Insert Plot: \\ Error Distribution (MEE Hist) \end{minipage}}
			%\caption{MEE Distribution (CV)}
			
			\column{0.5\textwidth}
			\centering
			% [PLACEHOLDER]
			\fbox{\begin{minipage}[c][4cm]{5cm} \centering Insert Plot: \\ Predicted vs True $z$ \\ (Parity Plot) \end{minipage}}
			%\caption{Reconstruction of $z$}
		\end{columns}
	\end{frame}
	
	% --- SLIDE 16: Discussion - Insights ---
	\begin{frame}{6. Discussion: Why this works?}
		\textbf{Geometric Insight vs "Black Box":}
		\begin{itemize}
			\item Neural Networks struggled because the mapping $x \to y$ is highly sensitive to noise in $x$ without explicit knowledge of the constraint $z$.
			\item The \textbf{Inverse Approach} ($z \to x$) exploits the fact that $x$ is generated from $z$.
			\item By filtering inputs based on their "reliability" (weights $w_i$), we ignore noisy dimensions and focus on those that strongly correlate with the manifold.
		\end{itemize}
		
		\textbf{Novelty Impact:}
		\begin{itemize}
			\item MEE reduction from $\approx 20$ (Naive) to $\mathbf{8.5}$ (Geometric).
		\end{itemize}
	\end{frame}
	
	% --- SLIDE 17: Discussion - Robustness ---
	\begin{frame}{6.1 Discussion: Robustness \& Generalization}
		\textbf{Leakage Check:}
		\begin{itemize}
			\item Geometric dependencies were verified on strict subsets of data. The equations hold universally.
		\end{itemize}
		
		\textbf{Efficiency:}
		\begin{itemize}
			\item The model is extremely lightweight (only storing coefficients for LBE).
			\item Newton refinement converges in 3-5 iterations.
		\end{itemize}
		
		\textbf{Critical Remarks:}
		\begin{itemize}
			\item Performance is bound by the noise level of the "cleanest" inputs.
			\item The model assumes the test set follows the same geometric generation process (which is true for CUP).
		\end{itemize}
	\end{frame}
	
	% --- SLIDE 18: Conclusions ---
	\begin{frame}{7. Conclusions}
		\textbf{Summary:}
		\begin{itemize}
			\item \textbf{MONK:} Confirmed that procedural complexity handling implies success on simpler tasks.
			\item \textbf{CUP:} Demonstrated that domain analysis (geometry) can vastly outperform black-box optimization. 
			\item Achieved State-of-the-Art performance (for this project scope) with MEE $\approx 8.5$.
		\end{itemize}
		
		\vspace{1cm}
		\textbf{Blind Test Results:}
		\begin{itemize}
			\item Filename: \texttt{[Surname1\_Surname2]\_CUP\_TS.csv}
			\item Nickname: \textbf{[Your Nickname]}
		\end{itemize}
	\end{frame}
	
	% --- APPENDIX ---
	\begin{frame}{Appendix A: Additional Plots}
		\centering
		\textit{Use this section for extra plots (e.g., Fourier spectrum details, residual plots per variable).}
	\end{frame}
	
	\end{document}